import os
import logging
from aiogram import Bot, Dispatcher, executor, types
from aiogram.contrib.fsm_storage.memory import MemoryStorage
from aiogram.dispatcher import FSMContext
from aiogram.dispatcher.filters.state import State, StatesGroup
from openai import OpenAI

# ---------- ENV ----------
BOT_TOKEN = os.getenv("BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not BOT_TOKEN:
    raise RuntimeError("‚ùå BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω")
if not OPENAI_API_KEY:
    raise RuntimeError("‚ùå OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")

# ---------- LOGGING ----------
logging.basicConfig(level=logging.INFO)

# ---------- INIT ----------
bot = Bot(token=BOT_TOKEN)
dp = Dispatcher(bot, storage=MemoryStorage())
client = OpenAI(api_key=OPENAI_API_KEY)

# ---------- STATES ----------
class TextGen(StatesGroup):
    topic = State()
    style = State()
    length = State()

# ---------- KEYBOARDS ----------
def main_menu():
    kb = types.ReplyKeyboardMarkup(resize_keyboard=True)
    kb.add("üß† –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç")
    return kb

def after_text_kb():
    kb = types.ReplyKeyboardMarkup(resize_keyboard=True)
    kb.add("üîÑ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –µ—â—ë")
    kb.add("üè† –í –º–µ–Ω—é")
    return kb

# ---------- START ----------
@dp.message_handler(commands=["start"])
async def start(message: types.Message):
    await message.answer(
        "–ü—Ä–∏–≤–µ—Ç üëã\n\n–Ø –º–æ–≥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç **–Ω–∞ –ª—é–±—É—é —Ç–µ–º—É**.\n"
        "–ù–∞–∂–º–∏ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ üëá",
        reply_markup=main_menu()
    )

# ---------- MENU ----------
@dp.message_handler(lambda m: m.text == "üè† –í –º–µ–Ω—é", state="*")
async def back_to_menu(message: types.Message, state: FSMContext):
    await state.finish()
    await start(message)

# ---------- START GENERATION ----------
@dp.message_handler(lambda m: m.text in ["üß† –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç", "üîÑ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –µ—â—ë"])
async def ask_topic(message: types.Message):
    await TextGen.topic.set()
    await message.answer("üìå –ù–∞–ø–∏—à–∏ —Ç–µ–º—É —Ç–µ–∫—Å—Ç–∞\n\n–ù–∞–ø—Ä–∏–º–µ—Ä:\n‚Ä¢ –º–æ—Ç–∏–≤–∞—Ü–∏—è\n‚Ä¢ –±–∏–∑–Ω–µ—Å\n‚Ä¢ –æ—Ç–Ω–æ—à–µ–Ω–∏—è\n‚Ä¢ —Ñ–∏–ª–æ—Å–æ—Ñ–∏—è")

# ---------- TOPIC ----------
@dp.message_handler(state=TextGen.topic)
async def get_topic(message: types.Message, state: FSMContext):
    await state.update_data(topic=message.text)
    await TextGen.next()
    await message.answer("üé≠ –í –∫–∞–∫–æ–º —Å—Ç–∏–ª–µ –ø–∏—Å–∞—Ç—å?\n\n–ù–∞–ø—Ä–∏–º–µ—Ä:\n‚Ä¢ –∂—ë—Å—Ç–∫–æ\n‚Ä¢ –º–æ—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–æ\n‚Ä¢ —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏\n‚Ä¢ –∏—Ä–æ–Ω–∏—á–Ω–æ")

# ---------- STYLE ----------
@dp.message_handler(state=TextGen.style)
async def get_style(message: types.Message, state: FSMContext):
    await state.update_data(style=message.text)
    await TextGen.next()
    await message.answer("üìè –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞?\n\n–ù–∞–ø—Ä–∏–º–µ—Ä:\n‚Ä¢ –∫–æ—Ä–æ—Ç–∫–æ\n‚Ä¢ —Å—Ä–µ–¥–Ω–µ\n‚Ä¢ –¥–ª–∏–Ω–Ω–æ")

# ---------- LENGTH + GENERATION ----------
@dp.message_handler(state=TextGen.length)
async def generate_text(message: types.Message, state: FSMContext):
    data = await state.get_data()

    topic = data["topic"]
    style = data["style"]
    length = message.text

    prompt = (
        f"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–µ–º—É: {topic}.\n"
        f"–°—Ç–∏–ª—å: {style}.\n"
        f"–î–ª–∏–Ω–∞: {length}.\n\n"
        "–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∂–∏–≤—ã–º, —Ü–µ–ø–ª—è—é—â–∏–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º."
    )

    await message.answer("‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–µ–∫—Å—Ç...")

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–≤—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤."},
            {"role": "user", "content": prompt}
        ]
    )

    text = response.choices[0].message.content

    await message.answer(
        f"‚ú® **–ì–æ—Ç–æ–≤–æ:**\n\n{text}",
        reply_markup=after_text_kb(),
        parse_mode="Markdown"
    )

    await state.finish()

# ---------- RUN ----------
if __name__ == "__main__":
    executor.start_polling(dp, skip_updates=True)
